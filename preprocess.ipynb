{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def convert_idx(text, tokens):\n",
    "    current = 0\n",
    "    spans = []\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current)\n",
    "        if current < 0:\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise Exception()\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token)\n",
    "    return spans\n",
    "\n",
    "def process_file(filename, data_type, word_counter, char_counter):\n",
    "    print(\"Generating {} examples...\".format(data_type))\n",
    "    examples = []\n",
    "    eval_examples = {}\n",
    "    total = 0\n",
    "    unans = 0\n",
    "    ans =0\n",
    "    with open(filename, \"r\") as fh:\n",
    "        source = json.load(fh)\n",
    "        for article in tqdm(source[\"data\"]):\n",
    "            for para in article[\"paragraphs\"]:\n",
    "                context = para[\"context\"].replace(\n",
    "                    \"''\", '\" ').replace(\"``\", '\" ')\n",
    "                context_tokens = word_tokenize(context)\n",
    "                context_chars = [list(token) for token in context_tokens]\n",
    "                spans = convert_idx(context, context_tokens)\n",
    "                for token in context_tokens:\n",
    "                    word_counter[token] += len(para[\"qas\"])\n",
    "                    for char in token:\n",
    "                        char_counter[char] += len(para[\"qas\"])\n",
    "                for qa in para[\"qas\"]:\n",
    "                    total += 1\n",
    "                    ques = qa[\"question\"].replace(\n",
    "                        \"''\", '\" ').replace(\"``\", '\" ')\n",
    "                    ques_tokens = word_tokenize(ques)\n",
    "                    ques_chars = [list(token) for token in ques_tokens]\n",
    "                    for token in ques_tokens:\n",
    "                        word_counter[token] += 1\n",
    "                        for char in token:\n",
    "                            char_counter[char] += 1\n",
    "                    y1s, y2s = [], []\n",
    "                    answer_texts = []\n",
    "                    \n",
    "                    # 2.0 Dataset\n",
    "                    if 'is_impossible' in qa and qa['is_impossible']==True:\n",
    "                        y1s.append(-1)\n",
    "                        y2s.append(-1)\n",
    "                        unans+=1\n",
    "                    else:\n",
    "                        ans+=1\n",
    "                        for answer in qa[\"answers\"]:\n",
    "                            answer_text = answer[\"text\"]\n",
    "                            answer_start = answer['answer_start']\n",
    "                            answer_end = answer_start + len(answer_text)\n",
    "                            answer_texts.append(answer_text)\n",
    "                            answer_span = []\n",
    "                            for idx, span in enumerate(spans):\n",
    "                                if not (answer_end <= span[0] or answer_start >= span[1]):\n",
    "                                    answer_span.append(idx)\n",
    "                            if len(answer_span)==0:\n",
    "                                print(answer)\n",
    "                            y1, y2 = answer_span[0], answer_span[-1]\n",
    "                            y1s.append(y1)\n",
    "                            y2s.append(y2)\n",
    "                    example = {\"context_tokens\": context_tokens, \"context_chars\": context_chars,\n",
    "                               \"ques_tokens\": ques_tokens,\n",
    "                               \"ques_chars\": ques_chars, \"y1s\": y1s, \"y2s\": y2s, \"id\": total}\n",
    "                    examples.append(example)\n",
    "                    eval_examples[str(total)] = {\"question\":ques,\n",
    "                        \"context\": context, \"spans\": spans, \"answers\": answer_texts, \"uuid\": qa[\"id\"]}\n",
    "        print(\"{} questions in total\".format(len(examples)))\n",
    "        print('answerable:',ans,'unanswerable:',unans)\n",
    "    return examples, eval_examples\n",
    "\n",
    "def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None):\n",
    "    print(\"Generating {} embedding...\".format(data_type))\n",
    "    embedding_dict = {}\n",
    "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
    "    if emb_file is not None:\n",
    "        assert size is not None\n",
    "        assert vec_size is not None\n",
    "        with open(emb_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "            for line in tqdm(fh, total=size):\n",
    "                array = line.split()\n",
    "                word = \"\".join(array[0:-vec_size])\n",
    "                vector = list(map(float, array[-vec_size:]))\n",
    "                if word in counter and counter[word] > limit:\n",
    "                    embedding_dict[word] = vector\n",
    "        print(\"{} / {} tokens have corresponding {} embedding vector\".format(\n",
    "            len(embedding_dict), len(filtered_elements), data_type))\n",
    "    else:\n",
    "        assert vec_size is not None\n",
    "        for token in filtered_elements:\n",
    "            embedding_dict[token] = [np.random.normal(\n",
    "                scale=0.1) for _ in range(vec_size)]\n",
    "        print(\"{} tokens have corresponding embedding vector\".format(\n",
    "            len(filtered_elements)))\n",
    "\n",
    "    NULL = \"--NULL--\"\n",
    "    OOV = \"--OOV--\"\n",
    "    token2idx_dict = {token: idx for idx,\n",
    "                      token in enumerate(embedding_dict.keys(), 2)}\n",
    "    token2idx_dict[NULL] = 0\n",
    "    token2idx_dict[OOV] = 1\n",
    "    embedding_dict[NULL] = [0. for _ in range(vec_size)]\n",
    "    embedding_dict[OOV] = [0. for _ in range(vec_size)] # np.random.random((vec_size))/2-0.25\n",
    "    idx2emb_dict = {idx: embedding_dict[token]\n",
    "                    for token, idx in token2idx_dict.items()}\n",
    "    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "    return emb_mat, token2idx_dict, idx2emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [01:24<00:00,  5.24it/s]\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599 questions in total\n",
      "answerable: 87599 unanswerable: 0\n",
      "Generating test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:10<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570 questions in total\n",
      "answerable: 10570 unanswerable: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "word_counter, char_counter = Counter(), Counter()\n",
    "\n",
    "# # 2.0 Dataset\n",
    "# test_examples, test_eval = process_file('original_data/dev-v2.0.json', \"test\", word_counter, char_counter)\n",
    "# train_examples, train_eval = process_file('original_data/train-v2.0.json', \"train\", word_counter, char_counter)\n",
    "\n",
    "# 1.0 Dataset\n",
    "train_examples, train_eval = process_file('../../fwei/data/squad/train-v1.1.json', \"train\", word_counter, char_counter)\n",
    "# dev_examples, dev_eval = process_file('../../fwei/data/squad/dev-v1.2.json', \"dev\", word_counter, char_counter)\n",
    "test_examples, test_eval = process_file('../../fwei/data/squad/dev-v1.1.json', \"test\", word_counter, char_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save train_eval and dev_eval\n",
    "# # 2.0 Dataset\n",
    "# with open('dataset/train_eval.json', \"w\") as fh:\n",
    "#     json.dump(train_eval, fh)\n",
    "# with open('dataset/test_eval.json','w') as fh:\n",
    "#     json.dump(test_eval,fh)\n",
    "    \n",
    "# 1.0 Dataset\n",
    "with open('dataset1.0/train_eval.json', \"w\") as fh:\n",
    "    json.dump(train_eval, fh)\n",
    "# with open('dataset1.0/dev_eval.json','w') as fh:\n",
    "#     json.dump(dev_eval,fh)\n",
    "with open('dataset1.0/test_eval.json','w') as fh:\n",
    "    json.dump(test_eval,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1044/2200000 [00:00<03:30, 10429.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2196017/2200000 [03:22<00:00, 10834.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91587 / 111136 tokens have corresponding word embedding vector\n",
      "Generating char embedding...\n",
      "1425 tokens have corresponding embedding vector\n"
     ]
    }
   ],
   "source": [
    "word_emb_mat, word2idx_dict, _ = get_embedding(\n",
    "    word_counter, \"word\", emb_file='../../fwei/data/glove/glove.840B.300d.txt', size=int(2.2e6), vec_size=300)\n",
    "char_emb_mat, char2idx_dict, _ = get_embedding(\n",
    "        char_counter, \"char\", emb_file=None, size=None, vec_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# id2word\n",
    "id2word_dict={}\n",
    "for k in word2idx_dict:\n",
    "    id2word_dict[word2idx_dict[k]]=k\n",
    "id2word_dict[0]=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91589\n",
      "1427\n",
      "(91589, 300)\n",
      "(1427, 64)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "word_size=len(word_emb_mat)\n",
    "char_input_size=len(char_emb_mat)\n",
    "print(word_size)\n",
    "print(char_input_size)\n",
    "\n",
    "# save w2id\n",
    "with open('dataset1.0/word2id.pkl','wb') as f:\n",
    "    pickle.dump(word2idx_dict,f)\n",
    "# save c2id   \n",
    "with open('dataset1.0/char2id.pkl','wb') as f:\n",
    "    pickle.dump(char2idx_dict,f)\n",
    "\n",
    "# save word_mat\n",
    "word_mat=np.zeros((len(word_emb_mat),len(word_emb_mat[0])))\n",
    "for i,w in enumerate(word_emb_mat):\n",
    "    word_mat[i,:]=w\n",
    "print(word_mat.shape)\n",
    "# np.save('dataset/word_emb_mat.npy',word_mat)\n",
    "np.save('dataset1.0/word_emb_mat.npy',word_mat)\n",
    "\n",
    "# save char_mat\n",
    "char_mat=np.zeros((len(char_emb_mat),len(char_emb_mat[0])))\n",
    "for i,w in enumerate(char_emb_mat):\n",
    "    char_mat[i,:]=w\n",
    "print(char_mat.shape)\n",
    "# np.save('dataset/char_emb_mat.npy',char_mat)\n",
    "np.save('dataset1.0/char_emb_mat.npy',char_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 98/87599 [00:00<01:30, 971.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [01:18<00:00, 1122.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 87358 / 87599 instances of features in total\n",
      "unanswerable:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 167/10570 [00:00<00:06, 1662.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      "Processing dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [00:09<00:00, 1091.07it/s]\n",
      "  1%|          | 105/10570 [00:00<00:10, 1044.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 10482 / 10570 instances of features in total\n",
      "unanswerable: 0\n",
      "Processing test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [00:11<00:00, 940.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 10570 / 10570 instances of features in total\n",
      "unanswerable: 0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def build_features(config, examples, data_type, out_file, word2idx_dict, char2idx_dict, id2word_dict, \\\n",
    "                   is_test=False):\n",
    "\n",
    "    para_limit = config['test_para_limit'] if is_test else config['para_limit']\n",
    "    ques_limit = config['test_ques_limit'] if is_test else config['ques_limit']\n",
    "    ans_limit = 100 if is_test else config['ans_limit']\n",
    "    char_limit = config['char_limit']\n",
    "\n",
    "    def filter_func(example, is_test=False):\n",
    "        return len(example[\"context_tokens\"]) > para_limit or \\\n",
    "               len(example[\"ques_tokens\"]) > ques_limit or \\\n",
    "               (example[\"y2s\"][0] - example[\"y1s\"][0]) > ans_limit\n",
    "\n",
    "    print(\"Processing {} examples...\".format(data_type))\n",
    "    total = 0\n",
    "    total_ = 0\n",
    "    meta = {}\n",
    "    context_idxss=[]\n",
    "    ques_idxss=[]\n",
    "    context_char_idxss=[]\n",
    "    ques_char_idxss=[]\n",
    "    context_strings=[]\n",
    "    ques_strings=[]\n",
    "    y1s=[]\n",
    "    y2s=[]\n",
    "    qids=[]\n",
    "    unans=0\n",
    "    for example in tqdm(examples):\n",
    "        total_ += 1\n",
    "\n",
    "        if filter_func(example, is_test):\n",
    "            continue\n",
    "\n",
    "        total += 1\n",
    "        qids.append(int(example['id']))\n",
    "        context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "        context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "        ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "        ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "        if config['data_ver']==2:\n",
    "            y1 = np.zeros([para_limit+1], dtype=np.float32)\n",
    "            y2 = np.zeros([para_limit+1], dtype=np.float32)\n",
    "        else:\n",
    "            y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "            y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "\n",
    "        def _get_word(word):\n",
    "            for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "                if each in word2idx_dict:\n",
    "                    return word2idx_dict[each]\n",
    "            return 1\n",
    "\n",
    "        def _get_char(char):\n",
    "            if char in char2idx_dict:\n",
    "                return char2idx_dict[char]\n",
    "            return 1\n",
    "        \n",
    "        cont_temp=[]\n",
    "        ques_temp=[]\n",
    "        for i, token in enumerate(example[\"context_tokens\"]):\n",
    "            context_idxs[i] = _get_word(token)\n",
    "            cont_temp.append(id2word_dict[context_idxs[i]])\n",
    "        while len(cont_temp)<para_limit:\n",
    "            cont_temp.append('')\n",
    "\n",
    "        for i, token in enumerate(example[\"ques_tokens\"]):\n",
    "            ques_idxs[i] = _get_word(token)\n",
    "            ques_temp.append(id2word_dict[ques_idxs[i]])\n",
    "        while len(ques_temp)<ques_limit:\n",
    "            ques_temp.append('')\n",
    "\n",
    "        for i, token in enumerate(example[\"context_chars\"]):\n",
    "            for j, char in enumerate(token):\n",
    "                if j == char_limit:\n",
    "                    break\n",
    "                context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "        for i, token in enumerate(example[\"ques_chars\"]):\n",
    "            for j, char in enumerate(token):\n",
    "                if j == char_limit:\n",
    "                    break\n",
    "                ques_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "        start, end = example[\"y1s\"][-1], example[\"y2s\"][-1]\n",
    "        if config['data_ver']==2:    \n",
    "            if start!=-1 and end!=-1:\n",
    "                y1[start+1], y2[end+1] = 1.0, 1.0\n",
    "            else:\n",
    "                y1[0], y2[0] = 1.0, 1.0\n",
    "                unans+=1\n",
    "        else:\n",
    "            y1[start], y2[end] = 1.0, 1.0\n",
    "        context_idxss.append(np.expand_dims(context_idxs,axis=0))\n",
    "        ques_idxss.append(np.expand_dims(ques_idxs,axis=0))\n",
    "        context_char_idxss.append(np.expand_dims(context_char_idxs,axis=0))\n",
    "        ques_char_idxss.append(np.expand_dims(ques_char_idxs,axis=0))\n",
    "        y1s.append(np.expand_dims(y1,axis=0))\n",
    "        y2s.append(np.expand_dims(y2,axis=0))\n",
    "        context_strings.append(cont_temp)\n",
    "        ques_strings.append(ques_temp)\n",
    "        \n",
    "    context_idxss=np.concatenate(context_idxss,axis=0)\n",
    "    ques_idxss=np.concatenate(ques_idxss,axis=0)\n",
    "    context_char_idxss=np.concatenate(context_char_idxss,axis=0)\n",
    "    ques_char_idxss=np.concatenate(ques_char_idxss,axis=0)\n",
    "    y1s=np.concatenate(y1s,axis=0)\n",
    "    y2s=np.concatenate(y2s,axis=0)\n",
    "    qids=np.array(qids)\n",
    "    context_strings=np.array(context_strings)\n",
    "    ques_strings=np.array(ques_strings)\n",
    "    \n",
    "    np.save(out_file+data_type+'_contw_input.npy',context_idxss)\n",
    "    np.save(out_file+data_type+'_quesw_input.npy',ques_idxss)\n",
    "    np.save(out_file+data_type+'_contc_input.npy',context_char_idxss)\n",
    "    np.save(out_file+data_type+'_quesc_input.npy',ques_char_idxss)\n",
    "    np.save(out_file+data_type+'_y_start.npy',y1s)\n",
    "    np.save(out_file+data_type+'_y_end.npy',y2s)\n",
    "    np.save(out_file+data_type+'_qid.npy',qids)\n",
    "    np.save(out_file+data_type+'_contw_strings.npy',context_strings)\n",
    "    np.save(out_file+data_type+'_quesw_strings.npy',ques_strings)\n",
    "    \n",
    "    print(\"Built {} / {} instances of features in total\".format(total, total_))\n",
    "    print('unanswerable:',unans)\n",
    "\n",
    "config={\n",
    "    'test_para_limit':1000,\n",
    "    'test_ques_limit':50,\n",
    "    'para_limit':400,\n",
    "    'ques_limit':50,\n",
    "    'ans_limit':30,\n",
    "    'char_limit':16,\n",
    "    'data_ver':1\n",
    "}\n",
    "\n",
    "# # 2.0 Dataset\n",
    "# build_features(config, train_examples, 'train', 'dataset/', word2idx_dict, char2idx_dict, id2word_dict, is_test=False)\n",
    "# build_features(config, test_examples, 'dev', 'dataset/', word2idx_dict, char2idx_dict, id2word_dict, is_test=False)\n",
    "# build_features(config, test_examples, 'test', 'dataset/', word2idx_dict, char2idx_dict, id2word_dict, is_test=True)\n",
    "\n",
    "# 1.0 Dataset\n",
    "build_features(config, train_examples, 'train', 'dataset1.0/', word2idx_dict, char2idx_dict, id2word_dict, is_test=False)\n",
    "build_features(config, test_examples, 'dev', 'dataset1.0/', word2idx_dict, char2idx_dict, id2word_dict, is_test=False)\n",
    "build_features(config, test_examples, 'test', 'dataset1.0/', word2idx_dict, char2idx_dict, id2word_dict, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将string转化为(context, sentence, words)格式并复合上词性tag, 子单元例如('IN','for')\n",
    "import spacy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_type='dev'\n",
    "cont_string=np.load(os.path.join('dataset',data_type+'_contw_strings.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MD', 'RB']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "print([i.tag_ for i in nlp('cannot')])\n",
    "# words=cont_string[0,:]\n",
    "# print(x)\n",
    "# tags_=[]\n",
    "# for w in words:\n",
    "#     wtag=[j.tag_ for j in nlp(str(w))]\n",
    "#     tags_.append(wtag)\n",
    "# print(words)\n",
    "# print(tags_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/ada/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 883, in __del__\n",
      "    self.close()\n",
      "  File \"/ada/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 1088, in close\n",
      "    self._decr_instances(self)\n",
      "  File \"/ada/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 439, in _decr_instances\n",
      "    cls._instances.remove(instance)\n",
      "  File \"/ada/anaconda3/lib/python3.6/_weakrefset.py\", line 109, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x7f670efd6f48; to 'tqdm' at 0x7f67122fa710>\n",
      "  1%|          | 13/1466 [00:04<08:17,  2.92it/s]]\n",
      " 85%|████████▍ | 1244/1466 [02:57<00:31,  7.00it/s][A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1335/1466 [03:01<00:17,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['...', 'You', \"'ve\", '[', 'Western', 'nations', ']', 'increased', 'the', 'price', 'of', 'the', 'wheat', 'you', 'sell', 'us', 'by', '300', 'percent', ',', 'and', 'the', 'same', 'for', 'sugar', 'and', 'cement', '...', 'You', 'buy', 'our', 'crude', 'oil', 'and', 'sell', 'it', 'back', 'to', 'us', ',', 'refined', 'as', 'petrochemicals', ',', 'at', 'a', 'hundred', 'times', 'the', 'price', 'you', \"'ve\", 'paid', 'us', '...', 'It', \"'s\", 'only', 'fair', 'that', ',', 'from', 'now', 'on', ',', 'you', 'should', 'pay', 'more', 'for', 'oil', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1466/1466 [03:20<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1466/1466 [03:32<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1466/1466 [03:32<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1468/1468 [03:32<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1466/1466 [03:42<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1466/1466 [03:45<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/ada/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"<ipython-input-10-dcf34bf605f0>\", line 25, in gettag\n    assert len(tags)==len(words)\nAssertionError\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dcf34bf605f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-dcf34bf605f0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/ada/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gettag(cont_string):\n",
    "    contexts=[]\n",
    "    wrong_num=0\n",
    "    for i in tqdm(range(cont_string.shape[0])):#range(cont_string.shape[0])\n",
    "        sentences=[]\n",
    "        words=[]\n",
    "        for j in range(cont_string.shape[1]):\n",
    "            if cont_string[i,j]=='':\n",
    "                break\n",
    "\n",
    "            # 规则矫正：\n",
    "            # 1.如果只有一个'，去除\n",
    "            if str(cont_string[i,j]).count('\\'')==1 and len(cont_string[i,j])>1:\n",
    "                cont_string[i,j]=cont_string[i,j].replace('\\'','')\n",
    "                \n",
    "            # 2.如果如果是cannot，改为not\n",
    "            if str(cont_string[i,j]).lower()=='cannot':\n",
    "                cont_string[i,j]='not'\n",
    "                \n",
    "            # 3.im，改为I\n",
    "            if str(cont_string[i,j]).lower()=='im':\n",
    "                cont_string[i,j]='i'\n",
    "                \n",
    "            # # 其余问题过滤（暂时）128K\n",
    "            # if str(cont_string[i,j])=='128K':\n",
    "            #     cont_string[i,j]='128'\n",
    "                \n",
    "            words.append(cont_string[i,j])\n",
    "            if words[-1]=='.' or words[-1]=='!' or words[-1]=='?':\n",
    "                sentence=' '.join(words)\n",
    "                tags=[n.tag_ for n in nlp(sentence)]\n",
    "                if len(tags)!=len(words):\n",
    "                    tags_=[]\n",
    "                    for w in words:\n",
    "                        wtag=[j.tag_ for j in nlp(str(w))]\n",
    "                        if len(wtag)>1:\n",
    "                            wtag=wtag[0]\n",
    "                        tags_.extend(wtag)\n",
    "                    tags=tags_\n",
    "                    wrong_num+=1\n",
    "                    assert len(tags)==len(words)\n",
    "                sentences.append(list(zip(tags,words)))\n",
    "                words=[]\n",
    "        if len(words)>0:\n",
    "            sentence=' '.join(words)\n",
    "            tags=[n.tag_ for n in nlp(sentence)]\n",
    "            if len(tags)!=len(words):\n",
    "                tags_=[]\n",
    "                for w in words:\n",
    "                    wtag=[j.tag_ for j in nlp(str(w))]\n",
    "                    if len(wtag)>1:\n",
    "                        wtag=wtag[0]\n",
    "                    tags_.extend(wtag)\n",
    "                tags=tags_\n",
    "                wrong_num+=1\n",
    "                assert len(tags)==len(words)\n",
    "            sentences.append(list(zip(tags,words)))\n",
    "            words=[]\n",
    "        contexts.append(sentences)\n",
    "    print(wrong_num)\n",
    "    \n",
    "    return contexts\n",
    "# contexts=gettag(cont_string)\n",
    "\n",
    "split_num=8\n",
    "temp_len=cont_string.shape[0]//split_num\n",
    "params=[]\n",
    "for i in range(split_num):\n",
    "    if i != split_num-1:\n",
    "        params.append(cont_string[i*temp_len:(i+1)*temp_len,::])\n",
    "    else:\n",
    "        params.append(cont_string[i*temp_len:,::])\n",
    "    \n",
    "from multiprocessing import Pool\n",
    "pool=Pool()\n",
    "result=[]\n",
    "for i in params:\n",
    "    result.append(pool.apply_async(gettag, kwds={'cont_string':i}))\n",
    "pool.close()\n",
    "pool.join()\n",
    "contexts=[]\n",
    "[contexts.extend(i.get()) for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_dropout': 0.2, 'char_lstm_input_dropout': 0.2, 'clip_grad_norm': 0.0, 'd_char_emb': 32, 'd_ff': 2048, 'd_kv': 64, 'd_label_hidden': 250, 'd_model': 1024, 'elmo_dropout': 0.5, 'embedding_dropout': 0.0, 'learning_rate': 0.0008, 'learning_rate_warmup_steps': 160, 'max_len_dev': 0, 'max_len_train': 0, 'morpho_emb_dropout': 0.2, 'num_heads': 8, 'num_layers': 4, 'num_layers_position_only': 0, 'partitioned': True, 'relu_dropout': 0.1, 'residual_dropout': 0.2, 'sentence_max_len': 400, 'step_decay': True, 'step_decay_factor': 0.5, 'step_decay_patience': 5, 'tag_emb_dropout': 0.2, 'timing_dropout': 0.0, 'use_chars_concat': False, 'use_chars_lstm': False, 'use_elmo': True, 'use_tags': False, 'use_words': False, 'word_emb_dropout': 0.4}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "While copying the parameter named embedding.position_table, whose dimensions in the model are torch.Size([400, 512]) and whose dimensions in the checkpoint are torch.Size([300, 512]).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/ada/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m                     \u001b[0mown_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor size, expected tensor [400 x 512] and src [300 x 512] to have the same number of elements, but got 204800 and 153600 elements respectively at /pytorch/torch/lib/TH/generic/THTensorCopy.c:86",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f3fe27bbfad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hparams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence_max_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hparams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_nk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNKChartParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/users/caochenjie/QANet_tf/parse_nk.py\u001b[0m in \u001b[0;36mfrom_spec\u001b[0;34m(cls, spec, model)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ada/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    517\u001b[0m                                        \u001b[0;34m'whose dimensions in the model are {} and '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                                        \u001b[0;34m'whose dimensions in the checkpoint are {}.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                                        .format(name, own_state[name].size(), param.size()))\n\u001b[0m\u001b[1;32m    520\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 raise KeyError('unexpected key \"{}\" in state_dict'\n",
      "\u001b[0;31mRuntimeError\u001b[0m: While copying the parameter named embedding.position_table, whose dimensions in the model are torch.Size([400, 512]) and whose dimensions in the checkpoint are torch.Size([300, 512])."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import torch\n",
    "import parse_nk\n",
    "torch.cuda.set_device(3)\n",
    "def torch_load(load_path):\n",
    "    if parse_nk.use_cuda:\n",
    "        return torch.load(load_path)\n",
    "    else:\n",
    "        return torch.load(load_path, map_location=lambda storage, location: storage)\n",
    "info = torch_load('parsing/models/en_elmo_dev.95.21.pt')\n",
    "assert 'hparams' in info['spec'], \"Older savefiles not supported\"\n",
    "info['spec']['hparams']['sentence_max_len']=400\n",
    "print(info['spec']['hparams'])\n",
    "parser = parse_nk.NKChartParser.from_spec(info['spec'], info['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('parsing/data/dev_tags.pkl','rb') as f:\n",
    "    tags=pickle.load(f)\n",
    "    tags=np.array(tags)\n",
    "tags_temp=tags[64:96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_parse_feat(tags_temp):\n",
    "    batch_size = len(tags_temp)\n",
    "    \n",
    "    # stastic the word num in each sample\n",
    "    sen_len=[sum([len(tt) for tt in t]) for t in tags_temp]\n",
    "    max_len=max(sen_len)\n",
    "    \n",
    "    # combine the sentences to a batch\n",
    "    tags_temp_new=[]\n",
    "    for i in range(len(tags_temp)):\n",
    "        combined_context=[]\n",
    "        [combined_context.extend(t) for t in tags_temp[i]]\n",
    "        tags_temp_new.append(combined_context)\n",
    "    print(len(tags_temp_new[11]))\n",
    "    # inference the parsing feature\n",
    "    feat,idxs = parser.parse_batch(tags_temp_new)\n",
    "    \n",
    "    # remove the elmo useless token from feat\n",
    "    inds=[]\n",
    "    for j in range(len(idxs.batch_idxs_np)):\n",
    "        if j==0 or j==len(idxs.batch_idxs_np)-1 or \\\n",
    "        idxs.batch_idxs_np[j-1]!=idxs.batch_idxs_np[j] or \\\n",
    "        idxs.batch_idxs_np[j+1]!=idxs.batch_idxs_np[j]:\n",
    "            continue\n",
    "        else:\n",
    "            inds.append(j)\n",
    "    feat=feat[inds,:]\n",
    "    \n",
    "    # convert feat to (batch_size, max_len, 1024)\n",
    "    assert sum(sen_len)==feat.shape[0]\n",
    "    feats=np.zeros((batch_size, max_len, 1024))\n",
    "    cusum=0\n",
    "    for i,s in enumerate(sen_len):\n",
    "        feats[i,0:s,:]=feat[cusum:cusum+s,:]\n",
    "        cusum+=s\n",
    "    assert cusum==feat.shape[0]\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'can', 'not', 'shoot', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]\n",
    "words = word_tokenize(\"im a footman.\")\n",
    "\n",
    "print(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
